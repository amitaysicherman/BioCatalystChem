import argparse
from torch.utils.data import DataLoader

import torch
import os
from rdkit import RDLogger

RDLogger.DisableLog('rdApp.*')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

from eval_gen import load_model_tokenizer_dataest, get_ec_from_df, tokens_to_canonical_smiles

import torch
from tqdm import tqdm


def eval_ensemble(models, tokenizers, dataloaders, max_length=200):
    pbar = tqdm(enumerate(zip(*dataloaders)), total=len(dataloaders[0]))

    for i, batches in pbar:
        input_ids_list = [batch['input_ids'].to(device) for batch in batches]
        attention_mask_list = [batch['attention_mask'].to(device).bool() for batch in batches]
        emb_list = [batch['emb'].to(device).float() for batch in batches]

        generated_tokens = [[] for _ in input_ids_list]  # To store tokens generated by each model

        for _ in range(max_length):
            ensemble_logits = None

            for model, input_ids, attention_mask, emb in zip(models, input_ids_list, attention_mask_list, emb_list):
                if (emb == 0).all():
                    emb_args = {}
                else:
                    emb_args = {"emb": emb}

                outputs = model(input_ids=input_ids, attention_mask=attention_mask, **emb_args)
                logits = outputs.logits[:, -1, :]
                if ensemble_logits is None:
                    ensemble_logits = logits
                else:
                    ensemble_logits += logits

            ensemble_logits /= len(models)

            next_token = torch.argmax(ensemble_logits, dim=-1)

            for idx, token in enumerate(next_token):
                generated_tokens[idx].append(token.item())
                input_ids_list[idx] = torch.cat([input_ids_list[idx], token.unsqueeze(0).unsqueeze(0)], dim=1)
            attention_mask_list = [torch.cat([mask, torch.ones((mask.size(0), 1), device=device)], dim=1) for mask in
                                   attention_mask_list]

        for tokens, tokenizer in zip(generated_tokens, tokenizers):
            final_smiles = tokens_to_canonical_smiles(tokenizer, tokens)
            print(f"Generated SMILES: {final_smiles}")

        # Update the progress bar
        pbar.set_description(f"Evaluated batch {i + 1}/{len(dataloaders[0])}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--run_names", default=['dae-0.5_5', 'pretrained_5', 'paper'], type=str, nargs='+')
    parser.add_argument("--split", default="valid", type=str)
    parser.add_argument("--per_level", default=1, type=int)

    args = parser.parse_args()
    run_names = args.run_names
    per_level = args.per_level

    print("---" * 10)
    print(f"Run: {run_names}")
    print("---" * 10)
    models, tokenizers, dataloaders = [], [], []
    dataset_len = None
    all_ec = None
    for run_name in args.run_names:
        model, tokenizer, gen_dataset = load_model_tokenizer_dataest(run_name, args.split, same_length=True)
        models.append(model)
        tokenizers.append(tokenizer)
        if dataset_len is None:
            dataset_len = len(gen_dataset)
        else:
            assert dataset_len == len(gen_dataset)
        if all_ec is None:
            all_ec = get_ec_from_df(gen_dataset, per_level)
        else:
            assert all_ec == get_ec_from_df(gen_dataset, per_level)

        dataloaders.append(DataLoader(gen_dataset, batch_size=1, num_workers=0))

    # Evaluate the averaged model
    os.makedirs("results/full", exist_ok=True)
    with torch.no_grad():
        eval_ensemble(models, tokenizers, dataloaders)
